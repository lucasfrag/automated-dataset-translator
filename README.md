# ğŸŒ Automated Dataset Translator

> Automatically translate structured datasets (CSV, JSON, JSONL, TSV, Parquet) using LLMs via Ollama â€” with caching, parallelism, checkpointing, and retry support.

---

## âœ¨ Features

* ğŸŒ Translate dataset **content automatically using LLMs**
* ğŸ“‚ Supports multiple formats:

  * CSV
  * JSON
  * JSONL
  * TSV
  * Parquet
* ğŸ§  Uses **local models via Ollama**
* âš¡ Parallel processing (multi-threaded)
* ğŸ’¾ Persistent cache (SQLite) â€” avoids retranslating identical text
* ğŸ” Automatic retry with exponential backoff
* â¸ï¸ Checkpoint system â€” resume interrupted translations
* ğŸ“Š Progress bars with tqdm
* ğŸ¯ Select specific columns to translate
* ğŸ”’ Safe and deterministic output generation

---

## ğŸ“¦ Installation

Clone the repository:

```bash
git clone https://github.com/lucasfrag/auto-dataset-translator.git
cd auto-dataset-translator
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Install Ollama:

https://ollama.ai

Pull a model:

```bash
ollama pull llama3.1:8b
```

---

## ğŸš€ Usage

Basic example:

```bash
python main.py \
  --input dataset.csv \
  --output dataset_pt.csv \
  --columns text title description \
  --model llama3.1:8b \
  --target-lang Portuguese
```

---

## âš¡ Parallel processing

Use multiple workers:

```bash
python main.py \
  -i dataset.csv \
  -o dataset_pt.csv \
  -c text title description \
  -m llama3.1:8b \
  -t Portuguese \
  -w 4
```

---

## ğŸ” Force retranslation

Ignore cache and checkpoint:

```bash
python main.py ... --force
```

---

## ğŸ§  How it works

Pipeline:

```
Load dataset
   â†“
Check cache
   â†“
Translate using Ollama
   â†“
Save to cache
   â†“
Save checkpoint
   â†“
Write output dataset
```

---

## ğŸ’¾ Cache system

Cache is stored in:

```
translation_cache.db
```

Benefits:

* Avoid retranslating identical text
* Massive performance improvements
* Persistent across runs

---

## â¸ï¸ Checkpoint system

Checkpoint is stored in:

```
checkpoint.db
```

Allows:

* Resume interrupted runs
* Process very large datasets safely
* Crash recovery

---

## ğŸ¯ Example

Input:

```csv
text,title
Hello world,Greeting
Machine learning is amazing,Statement
```

Output:

```csv
text,title
OlÃ¡ mundo,SaudaÃ§Ã£o
Aprendizado de mÃ¡quina Ã© incrÃ­vel,DeclaraÃ§Ã£o
```

---

## âš™ï¸ Arguments

| Argument              | Description                 |
| --------------------- | --------------------------- |
| `--input`, `-i`       | Input dataset               |
| `--output`, `-o`      | Output dataset              |
| `--columns`, `-c`     | Columns to translate        |
| `--model`, `-m`       | Ollama model                |
| `--target-lang`, `-t` | Target language             |
| `--source-lang`, `-s` | Source language (optional)  |
| `--workers`, `-w`     | Parallel workers            |
| `--force`             | Ignore cache and checkpoint |
| `--reset-cache`       | Delete cache                |
| `--reset-checkpoint`  | Delete checkpoint           |
| `--max-retries`       | Retry attempts              |
| `--retry-delay`       | Base retry delay            |

---

## âš¡ Performance

Features designed for scalability:

* Parallel processing
* Persistent caching
* Checkpoint resume
* Thread-safe SQLite backend

---

## ğŸ› ï¸ Requirements

* Python 3.9+
* Ollama

---

## ğŸ§  Recommended Models for Translation (Ollama)

The following models are fully compatible with Ollama and provide excellent multilingual translation performance.

| Model | Parameters | Quality | Speed | RAM Required | Recommendation | Notes |
|------|------------|---------|-------|--------------|----------------|------|
| **qwen3:14b** â­ | 14B | â­â­â­â­â­ | â­â­â­â­ | 16â€“24 GB | ğŸ¥‡ Best overall | Best balance of quality and performance |
| **qwen3:32b** | 32B | â­â­â­â­â­ | â­â­â­ | 32â€“48 GB | ğŸ¥‡ Best quality | Highest translation accuracy |
| **qwen3:8b** | 8B | â­â­â­â­ | â­â­â­â­â­ | 8â€“12 GB | ğŸ¥‡ Best for laptops | Fast and efficient |
| **mixtral:8x7b** | 46B (MoE) | â­â­â­â­â­ | â­â­â­ | 32+ GB | ğŸ¥‡ Production use | Extremely strong multilingual performance |
| **gemma3:27b** | 27B | â­â­â­â­â­ | â­â­â­ | 24â€“32 GB | ğŸ¥‡ Excellent alternative | Very stable translations |
| **command-r-plus** | 104B | â­â­â­â­â­ | â­â­ | 48+ GB | ğŸ¥‡ Enterprise | Best instruction-following |
| **gemma3:12b** | 12B | â­â­â­â­ | â­â­â­â­ | 12â€“16 GB | ğŸ¥ˆ Recommended | Great balance |
| **mistral-small3.2** | 24B | â­â­â­â­ | â­â­â­â­ | 16â€“24 GB | ğŸ¥ˆ Recommended | Fast and reliable |
| **phi3.5:medium** | 14B | â­â­â­â­ | â­â­â­â­ | 12â€“16 GB | ğŸ¥ˆ Lightweight | Efficient and capable |
| **qwen3:4b** | 4B | â­â­â­ | â­â­â­â­â­ | 6â€“8 GB | ğŸ¥‰ Lightweight | Good for low-resource systems |

---

## ğŸ¥‡ Best Model by Hardware

| Hardware | Recommended Model |
|--------|------------------|
| 8 GB RAM | qwen3:4b |
| 16 GB RAM | qwen3:8b â­ |
| 24 GB RAM | qwen3:14b â­â­â­ |
| 32 GB RAM | gemma3:27b or mixtral:8x7b |
| 48+ GB RAM | qwen3:32b or command-r-plus |

---

## ğŸš€ Installation example

```bash
ollama pull qwen3:14b
```
---

## ğŸ“œ License

MIT License

---

## â­ If you like this project, consider giving it a star!
